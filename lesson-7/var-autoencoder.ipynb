{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A convolutional autoencoder in PyTorch\n",
    "\n",
    "**[Faisal Z. Qureshi](http://vclab.science.uoit.ca)**  \n",
    "\n",
    "Check out excellent PyTorch tutorials by \"SherlockLiao\" at [https://github.com/L1aoXingyu/pytorch-beginner](https://github.com/L1aoXingyu/pytorch-beginner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See if have cuda support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU device found\n"
     ]
    }
   ],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "    print(\"Found device:\", torch.cuda.get_device_name(i))\n",
    "\n",
    "if torch.cuda.device_count() == 0:\n",
    "    print(\"No GPU device found\")\n",
    "else:\n",
    "    print(\"Current cuda device is\", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "my_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset = MNIST('../datasets', transform=my_transforms, download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 1, 28, 28)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Variational Autoencoder\n",
    "\n",
    "### `reparametrize(self, mu, logvar)`\n",
    "\n",
    "`logvar` = $\\log \\sigma^2$    \n",
    "`mu` = $\\mu$\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\sigma & = & \\exp \\left( \\frac{1}{2} \\log \\sigma^2 \\right) \\\\\n",
    "& = & \\exp \\left( \\log \\sigma^{2 ^ \\frac{1}{2}} \\right) \\\\\n",
    "& = & \\exp \\log \\sigma \\\\\n",
    "& = & \\sigma\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "How to generate a random from normal distribution with mean $\\mu$ and variance $\\sigma^2$?\n",
    "\n",
    "1. Generate $x \\sim \\mathcal{N}(0,1)$.\n",
    "2. Compute $y = \\sigma x + \\mu$.  Here $y \\sim \\mathcal{N}(\\mu,\\sigma^2)$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class var_autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(var_autoencoder, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400)       # These layers will be used for encoding \n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)        # These layers will be used for decoding\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))             # h1 is 400-dimensional vector\n",
    "        return self.fc21(h1), self.fc22(h1)  # this returns two 20-dimensional vectors\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if torch.cuda.is_available():\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)         # Latent variable z is 20-dimensional \n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))             # z can be used to generate new samples\n",
    "        return F.sigmoid(self.fc4(h3))       # remember that z ~ N(mu, var)\n",
    "\n",
    "    def forward(self, x):                    \n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model = var_autoencoder().cuda() # Lets put this on Cuda\n",
    "else:\n",
    "    model = var_autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "weight_decay = 1e-5\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/1], loss:0.9230\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for data in dataloader:        \n",
    "        img, _ = data # img is a [batch_size, num_channels, 28, 28] tensor\n",
    "                      # here num_channels is 1\n",
    "        \n",
    "        img = img.view(img.size(0), -1) # We resize it to [batchsize, 1x28x28] tensor\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            img = Variable(img).cuda() # Lets put this on Cuda\n",
    "        else:\n",
    "            img = Variable(img)\n",
    "\n",
    "        output, mu, logvar = model(img) # Forward\n",
    "        loss = criterion(output, img)\n",
    "\n",
    "        optimizer.zero_grad() # Backward & update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('epoch [{}/{}], loss:{:.4f}'\n",
    "          .format(epoch + 1, num_epochs, loss.data.item()))\n",
    "    if epoch % 1 == 0:\n",
    "        pic = to_img(output.cpu().data)\n",
    "        save_image(pic, 'image-vae-{}.png'.format(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the trained model\n",
    "\n",
    "Now that training is done, it is a good idea to save the trained model.\n",
    "\n",
    "We are interested in state_dict dictionary that contains parameters associated with each layer in the model.  Optimizer too has a state_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "fc1.weight  --  torch.Size([400, 784])\n",
      "fc1.bias  --  torch.Size([400])\n",
      "fc21.weight  --  torch.Size([20, 400])\n",
      "fc21.bias  --  torch.Size([20])\n",
      "fc22.weight  --  torch.Size([20, 400])\n",
      "fc22.bias  --  torch.Size([20])\n",
      "fc3.weight  --  torch.Size([400, 20])\n",
      "fc3.bias  --  torch.Size([400])\n",
      "fc4.weight  --  torch.Size([784, 400])\n",
      "fc4.bias  --  torch.Size([784])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \" -- \" , model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer's state_dict:\n",
      "state\n",
      "param_groups\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name)\n",
    "    #print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'conv-ae-weights.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Now lets pass an image through the learned model and see what we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x124946f60>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = dataset[1]\n",
    "print(image.shape, label)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(image[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 784]) torch.Size([1, 20]) torch.Size([1, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x124b0e9e8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ni = (image - 0.5)/0.5\n",
    "ni = ni.view(28*28).unsqueeze_(0)\n",
    "a, b, c = model(ni)\n",
    "print(a.shape, b.shape, c.shape)\n",
    "oi_ = to_img(a.view(1,1,28,28))\n",
    "plt.imshow(oi_[0,0,:,:].detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the saved model\n",
    "\n",
    "We can easily load the model as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'autoencoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-94e3a8640da9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Lets put this on Cuda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'autoencoder' is not defined"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model2 = autoencoder().cuda() # Lets put this on Cuda\n",
    "else:\n",
    "    model2 = autoencoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following should produce garbage, since we haven't trained the model yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oi = model2(ni)\n",
    "#print(oi.shape)\n",
    "oi_ = to_img(oi)\n",
    "#print(oi_.shape)\n",
    "\n",
    "plt.imshow(oi_[0,0,:,:].detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets load the model and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.load_state_dict(torch.load('conv-ae-weights.pt'))\n",
    "oi = model2(ni)\n",
    "#print(oi.shape)\n",
    "oi_ = to_img(oi)\n",
    "#print(oi_.shape)\n",
    "\n",
    "plt.imshow(oi_[0,0,:,:].detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when you load the model all is well in the world of autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using part of the trained network\n",
    "​\n",
    "Now lets assume we are interested in the encoder bit only.  I.e., we want to pass an MNIST image and wants to get its 3-dimensional encoding.  We can do it as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(my_encoder, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(*list(model2.encoder.children())[:])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = my_encoder()\n",
    "encoding = encoder(ni)\n",
    "print(encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
